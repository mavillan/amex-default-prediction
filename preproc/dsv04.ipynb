{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel,delayed\n",
    "import time\n",
    "import re\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "#from pandarallel import pandarallel\n",
    "#pandarallel.initialize(progress_bar=True, use_memory_fs=True)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from memory import reduce_mem_usage\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"../data/processed/dsv04\")\n",
    "\n",
    "if not OUTPUT_PATH.exists():\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this datasets aggreates the features over the time dimension\n",
    "\n",
    "- takes as base this dataset: https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format\n",
    "- feat engineering from here: https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
    "- lag features idea from here: https://www.kaggle.com/code/thedevastator/lag-features-are-all-you-need/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def compute_slope(x, y):\n",
    "    x_mean = x.mean()\n",
    "    y_mean = y.mean()\n",
    "    return np.sum((x-x_mean)*(y-y_mean)) / np.sum((x-x_mean)**2)\n",
    "\n",
    "def compute_slope_cols(df, customer_ID, num_features):\n",
    "    n = len(df)\n",
    "    if n > 2:\n",
    "        x = np.arange(n)\n",
    "        _df = df[num_features].fillna(method=\"ffill\", axis=0).fillna(method=\"bfill\", axis=0)\n",
    "        r = _df[num_features].apply(lambda y: compute_slope(x, y.values))\n",
    "        r = r.to_dict()\n",
    "    else:\n",
    "        r = df[num_features].apply(lambda y: 0)\n",
    "        r = r.to_dict()\n",
    "    r[\"customer_ID\"] = customer_ID\n",
    "    return r\n",
    "\n",
    "def mode_1st(x):\n",
    "    return x.value_counts().index[0]\n",
    "\n",
    "def mode_2nd(x):\n",
    "    try: return x.value_counts().index[1]\n",
    "    except: return -1 \n",
    "\n",
    "numba.njit()\n",
    "def compute_last_diff(array):\n",
    "    if len(array) <= 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return array[-1]-array[-2]\n",
    "    \n",
    "def compute_last_diff_series(df, col):\n",
    "    r = df.groupby(\"customer_ID\")[col].apply(lambda x: compute_last_diff(x.values))\n",
    "    r.name = f\"{r.name}_diff\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "# https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
    "# https://www.kaggle.com/code/cdeotte/xgboost-starter-0-793\n",
    "# after pay feats: https://www.kaggle.com/code/jiweiliu/rapids-cudf-feature-engineering-xgb\n",
    "# other lag features: https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977\n",
    "\n",
    "def remove_noise(df): \n",
    "    # removes noise from float columns\n",
    "    float_cols = df.dtypes[df.dtypes == \"float32\"].index\n",
    "    print(f\"# of float cols to reduce noise: {len(float_cols)}\")\n",
    "    \n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].round(decimals=2)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def build_features(df, ohe_cols):\n",
    "    \n",
    "    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n",
    "    num_features = [col for col in all_cols if col not in ohe_cols]\n",
    "    \n",
    "    print(\"Computing 'after pay' features\")\n",
    "    tic = time.time()\n",
    "    for bcol in [f'B_{i}' for i in [11,14,17]]+['D_39','D_131']+[f'S_{i}' for i in [16,23]]:\n",
    "        for pcol in ['P_2','P_3']:\n",
    "            if bcol in df.columns:\n",
    "                df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
    "                num_features.append(f'{bcol}-{pcol}')\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\\n\")\n",
    "    \n",
    "    print(\"Computing numerical aggregations\")\n",
    "    tic = time.time()\n",
    "    df_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    df_num_agg.columns = ['_'.join(x) for x in df_num_agg.columns]\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\\n\")\n",
    "    \n",
    "    print(\"Computing lag features\")\n",
    "    for col in num_features:\n",
    "        df_num_agg[f\"{col}_diff_wfirst\"] = df_num_agg[f\"{col}_last\"] - df_num_agg[f\"{col}_first\"]\n",
    "        df_num_agg[f\"{col}_diff_wmean\"] = df_num_agg[f\"{col}_last\"] - df_num_agg[f\"{col}_mean\"]        \n",
    "\n",
    "    to_remove = list(filter(re.compile(\".*_first\").match, df_num_agg.columns))\n",
    "    df_num_agg.drop(to_remove, axis=1, inplace=True)\n",
    "    \n",
    "    print(\"Computing diff features\")\n",
    "    results = list()\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "            delayed_func = delayed(compute_last_diff_series)\n",
    "            results = parallel(\n",
    "                delayed_func(df, col) \n",
    "                for col in tqdm(num_features)\n",
    "            )\n",
    "    df_diff = pd.concat(results, axis=1)\n",
    "    \n",
    "    print(\"Computing categorical aggregations\")\n",
    "    tic = time.time()\n",
    "    df_cat_agg1 = (\n",
    "        df\n",
    "        .groupby(\"customer_ID\")\n",
    "        [ohe_cols]\n",
    "        .mean()\n",
    "    )\n",
    "    df_cat_agg1.columns = [col+\"_\"+\"mean\" for col in df_cat_agg1.columns]\n",
    "    df_cat_agg2 = (\n",
    "        df\n",
    "        .groupby(\"customer_ID\")\n",
    "        [ohe_cols]\n",
    "        .agg(compute_last_observed)\n",
    "    )\n",
    "    df_cat_agg2.columns = [col+\"_\"+\"lo\" for col in df_cat_agg2.columns]\n",
    "    \n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\\n\")\n",
    "    \n",
    "    #print(\"Computing slope features\")\n",
    "    #tic = time.time()\n",
    "    #with Parallel(n_jobs=-1) as parallel:\n",
    "    #        delayed_func = delayed(compute_slope_cols)\n",
    "    #        results = parallel(\n",
    "    #            delayed_func(_df, customer_ID, num_features) \n",
    "    #            for customer_ID,_df in tqdm(df.groupby(\"customer_ID\"))\n",
    "    #        )\n",
    "    #slopes_df = pd.DataFrame(results).fillna(0).set_index(\"customer_ID\")\n",
    "    #slopes_df.columns = [f\"{col}_slope\" for col in slopes_df.columns]\n",
    "    #tac = time.time()\n",
    "    #print(f\"Elapsed time: {(tac-tic)/60} min\\n\")\n",
    "    \n",
    "    print(\"Building S_2 related features\")\n",
    "    tic = time.time()\n",
    "    df_count = df.groupby([\"customer_ID\"])[\"S_2\"].count()\n",
    "    df_count = pd.DataFrame(df_count).rename({\"S_2\":\"S_2_steps\"}, axis=1)\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\\n\")\n",
    "\n",
    "    print(\"Concatenating all the results\")\n",
    "    tic = time.time()\n",
    "    all_dfs = [df_num_agg, df_diff, df_cat_agg1, df_cat_agg2, df_count]\n",
    "    df = pd.concat(all_dfs, axis=1)\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\\n\")\n",
    "    \n",
    "    del df_num_agg, df_diff, df_cat_agg1, df_cat_agg2, df_count\n",
    "    gc.collect()\n",
    "\n",
    "    print('shape after engineering', df.shape )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categoricals(dataframe, encoder=None):\n",
    "    categoricals = [\n",
    "        'B_30', 'B_38', 'D_63', 'D_64', 'D_66', 'D_68', \n",
    "        'D_114', 'D_116', 'D_117', 'D_120', 'D_126',\n",
    "    ]\n",
    "    \n",
    "    if encoder is None:\n",
    "        print(\"fitting the encoder\")\n",
    "        encoder = ce.one_hot.OneHotEncoder(cols=categoricals)\n",
    "        encoder.fit(dataframe[categoricals])\n",
    "        \n",
    "    out = encoder.transform(dataframe[categoricals]).astype(np.int8)\n",
    "    ohe_cols = encoder.get_feature_names()\n",
    "    \n",
    "    dataframe.drop(categoricals, axis=1, inplace=True)\n",
    "    dataframe = pd.concat([dataframe, out], axis=1)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe, encoder, ohe_cols\n",
    "\n",
    "#@numba.njit()\n",
    "def compute_last_observed(series):\n",
    "    idx = np.nonzero(series.values[::-1])[0]\n",
    "    if len(idx)==0:\n",
    "        return 100\n",
    "    else:\n",
    "        return idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## preproc on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"S_2\"] = pd.to_datetime(train.S_2)\n",
    "#train[\"year\"] = train.S_2.dt.year\n",
    "#train[\"month\"] = train.S_2.dt.month\n",
    "\n",
    "#train[\"year_month\"] = train.year.astype(str) + \"-\" + train.month.astype(str)\n",
    "\n",
    "#cid = train.sample()[\"customer_ID\"].values[0]\n",
    "#df = train.query(\"customer_ID == @cid\")\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def diff_month(d1, d2):\n",
    "#    return (d1.year - d2.year) * 12 + d1.month - d2.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_antiquity(df):\n",
    "#    return (df.S_2.dt.year.values[-1] - df.S_2.dt.year.values[0])*12 + (df.S_2.dt.month.values[-1] - df.S_2.dt.month.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#r1 = train.groupby(\"customer_ID\").apply(compute_antiquity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#r2 = train.groupby(\"customer_ID\")[\"S_2\"].apply(lambda x: x.diff().max().days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../data/ext/amex-data-integer-dtypes-parquet-format/train.parquet\")\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = remove_noise(train)\n",
    "train, encoder, ohe_cols = encode_categoricals(train)\n",
    "train_agg = build_features(train, ohe_cols)\n",
    "train_agg = reduce_mem_usage(train_agg, verbose=True)\n",
    "train_agg.to_parquet(str(OUTPUT_PATH/\"train.parquet\"))\n",
    "\n",
    "del train,train_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## preproc on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"../data/ext/amex-data-integer-dtypes-parquet-format/test.parquet\")\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test = remove_noise(test)\n",
    "test, _, _ = encode_categoricals(test, encoder)\n",
    "test_agg = build_features(test, ohe_cols)\n",
    "test_agg = reduce_mem_usage(test_agg, verbose=True)\n",
    "test_agg.to_parquet(str(OUTPUT_PATH/\"test.parquet\"))\n",
    "\n",
    "del test,test_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
