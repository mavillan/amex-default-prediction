{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel,delayed\n",
    "import time\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import category_encoders as ce\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from memory import reduce_mem_usage\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=True, verbose=2, use_memory_fs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"../data/processed/dsv05\")\n",
    "\n",
    "if not OUTPUT_PATH.exists():\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this datasets aggreates the features over the time dimension\n",
    "\n",
    "- takes as base this dataset: https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format\n",
    "- feat engineering from here: https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
    "- lag features idea from here: https://www.kaggle.com/code/thedevastator/lag-features-are-all-you-need/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def compute_slope(x, y):\n",
    "    x_mean = x.mean()\n",
    "    y_mean = y.mean()\n",
    "    return np.sum((x-x_mean)*(y-y_mean)) / np.sum((x-x_mean)**2)\n",
    "\n",
    "def compute_slope_cols(df, customer_ID, num_features):\n",
    "    n = len(df)\n",
    "    if n > 2:\n",
    "        x = np.arange(n)\n",
    "        _df = df[num_features].fillna(method=\"ffill\", axis=0).fillna(method=\"bfill\", axis=0)\n",
    "        r = _df[num_features].apply(lambda y: compute_slope(x, y.values))\n",
    "        r = r.to_dict()\n",
    "    else:\n",
    "        r = df[num_features].apply(lambda y: 0)\n",
    "        r = r.to_dict()\n",
    "    r[\"customer_ID\"] = customer_ID\n",
    "    return r\n",
    "\n",
    "def mode_1st(x):\n",
    "    return x.value_counts().index[0]\n",
    "\n",
    "def mode_2nd(x):\n",
    "    try: return x.value_counts().index[1]\n",
    "    except: return -1 \n",
    "\n",
    "numba.njit()\n",
    "def compute_last_diff(array):\n",
    "    if len(array) <= 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return array[-1]-array[-2]\n",
    "    \n",
    "def compute_last_diff_series(df, col):\n",
    "    r = df.groupby(\"customer_ID\")[col].apply(lambda x: compute_last_diff(x.values))\n",
    "    r.name = f\"{r.name}_diff\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_position(df, customer_ID, num_features):\n",
    "    out = OrderedDict()\n",
    "    out[\"customer_ID\"] = customer_ID\n",
    "    for col in num_features:\n",
    "        idxmin = np.argmin(df[col].values[::-1])\n",
    "        idxmax = np.argmax(df[col].values[::-1])\n",
    "        out[f\"{col}_idxmin\"] = idxmin\n",
    "        out[f\"{col}_idxmax\"] = idxmax\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categoricals(dataframe, encoder=None):\n",
    "    categoricals = [\n",
    "        'B_30', 'B_38', 'D_63', 'D_64', 'D_66', 'D_68', \n",
    "        'D_114', 'D_116', 'D_117', 'D_120', 'D_126',\n",
    "    ]\n",
    "    \n",
    "    if encoder is None:\n",
    "        print(\"[INFO] fitting the encoder\")\n",
    "        encoder = ce.one_hot.OneHotEncoder(cols=categoricals)\n",
    "        encoder.fit(dataframe[categoricals])\n",
    "        \n",
    "    out = encoder.transform(dataframe[categoricals]).astype(np.int8)\n",
    "    ohe_cols = encoder.get_feature_names()\n",
    "    \n",
    "    dataframe.drop(categoricals, axis=1, inplace=True)\n",
    "    dataframe = pd.concat([dataframe, out], axis=1)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe, encoder, ohe_cols\n",
    "\n",
    "#@numba.njit()\n",
    "def compute_last_observed(series):\n",
    "    idx = np.nonzero(series.values[::-1])[0]\n",
    "    if len(idx)==0:\n",
    "        return 100\n",
    "    else:\n",
    "        return idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_consecutives(df, customer_ID, num_features):\n",
    "    out = OrderedDict()\n",
    "    out[\"customer_ID\"] = customer_ID\n",
    "    for col in num_features:\n",
    "        out[col] = np.sum(np.cumprod(df[col].values[::-1]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_month(d1, d2):\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "def compute_antiquity(df):\n",
    "    return (df.S_2.dt.year.values[-1] - df.S_2.dt.year.values[0])*12 + (df.S_2.dt.month.values[-1] - df.S_2.dt.month.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_nan_position(series):\n",
    "    idx = np.nonzero(series.values)\n",
    "    if len(idx)==0:\n",
    "        return 100\n",
    "    else:\n",
    "        return idx[0]\n",
    "    \n",
    "def last_nan_position(series):\n",
    "    idx = np.nonzero(series.values[::-1])\n",
    "    if len(idx)==0:\n",
    "        return 100\n",
    "    else:\n",
    "        return idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "# https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
    "# https://www.kaggle.com/code/cdeotte/xgboost-starter-0-793\n",
    "# after pay feats: https://www.kaggle.com/code/jiweiliu/rapids-cudf-feature-engineering-xgb\n",
    "# other lag features: https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977\n",
    "\n",
    "def remove_noise(df): \n",
    "    # removes noise from float columns\n",
    "    float_cols = df.dtypes[df.dtypes == \"float32\"].index\n",
    "    print(f\"[INFO] number of float cols to reduce noise: {len(float_cols)}\")\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].round(decimals=2)\n",
    "    return df\n",
    "\n",
    "def build_features(df, ohe_cols=None):\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"S_2\"] = pd.to_datetime(df.S_2)\n",
    "\n",
    "    n_customers = df[\"customer_ID\"].nunique()\n",
    "    \n",
    "    all_cols = [c for c in df.columns if c not in ['customer_ID','S_2']]\n",
    "    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "    if ohe_cols is None: ohe_cols = list()\n",
    "    num_features = [col for col in all_cols if col not in cat_features+ohe_cols]\n",
    "\n",
    "    all_results = list()\n",
    "    \n",
    "    print(\"[INFO] Computing 'after pay' features\")\n",
    "    tic = time.time()\n",
    "    for bcol in [f'B_{i}' for i in [11,14,17]]+['D_39','D_131']+[f'S_{i}' for i in [16,23]]:\n",
    "        for pcol in ['P_2','P_3']:\n",
    "            if bcol in df.columns:\n",
    "                df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
    "                num_features.append(f'{bcol}-{pcol}')\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "    \n",
    "    print(\"[INFO] Computing numerical aggregations\")\n",
    "    tic = time.time()\n",
    "    df_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'median', 'std', 'min', 'max', 'last'])\n",
    "    df_num_agg.columns = ['_'.join(x) for x in df_num_agg.columns]\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "    \n",
    "    print(\"[INFO] Computing diff_last-* features\")\n",
    "    tic = time.time()\n",
    "    for col in num_features:\n",
    "        df_num_agg[f\"{col}_diff_last-first\"] = df_num_agg[f\"{col}_last\"] - df_num_agg[f\"{col}_first\"]\n",
    "        df_num_agg[f\"{col}_diff_last-mean\"] = df_num_agg[f\"{col}_last\"] - df_num_agg[f\"{col}_mean\"]  \n",
    "        df_num_agg[f\"{col}_diff_last-median\"] = df_num_agg[f\"{col}_last\"] - df_num_agg[f\"{col}_median\"]\n",
    "        df_num_agg[f\"{col}_diff_last-min\"] = df_num_agg[f\"{col}_last\"] - df_num_agg[f\"{col}_min\"]\n",
    "        df_num_agg[f\"{col}_diff_last-max\"] = df_num_agg[f\"{col}_last\"] - df_num_agg[f\"{col}_max\"]\n",
    "\n",
    "    to_remove = list(filter(re.compile(\".*_first\").match, df_num_agg.columns))\n",
    "    df_num_agg.drop(to_remove, axis=1, inplace=True)\n",
    "\n",
    "    df_num_agg = reduce_mem_usage(df_num_agg, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_num_agg.shape[0]\n",
    "    all_results.append(df_num_agg)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "    \n",
    "    \n",
    "    print(\"[INFO] Computing above/below mean\")\n",
    "    tic = time.time()\n",
    "    \n",
    "    df_tmp = df.groupby(\"customer_ID\")[num_features].transform(\"median\")\n",
    "\n",
    "    df_above_mean = df[num_features] > df_tmp\n",
    "    df_above_mean[\"customer_ID\"] = df[\"customer_ID\"]\n",
    "\n",
    "    df_below_mean = df[num_features] < df_tmp\n",
    "    df_below_mean[\"customer_ID\"] = df[\"customer_ID\"]\n",
    "\n",
    "    df_above_mean_agg = df_above_mean.groupby(\"customer_ID\")[num_features].sum()\n",
    "    df_above_mean_agg.columns = [col+\"_above_mean\" for col in df_above_mean_agg.columns]\n",
    "\n",
    "    df_below_mean_agg = df_below_mean.groupby(\"customer_ID\")[num_features].sum()\n",
    "    df_below_mean_agg.columns = [col+\"_below_mean\" for col in df_below_mean_agg.columns]\n",
    "    \n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        delayed_func = delayed(count_consecutives)\n",
    "        results = parallel(\n",
    "           delayed_func(_df, customer_ID, num_features) \n",
    "           for customer_ID,_df in tqdm(df_above_mean.groupby(\"customer_ID\"))\n",
    "        )\n",
    "    df_above_mean_cc = pd.DataFrame(results).set_index(\"customer_ID\")\n",
    "    df_above_mean_cc.columns = [f\"{col}_above_mean_cc\" for col in df_above_mean_cc.columns]\n",
    "\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        delayed_func = delayed(count_consecutives)\n",
    "        results = parallel(\n",
    "           delayed_func(_df, customer_ID, num_features) \n",
    "           for customer_ID,_df in tqdm(df_below_mean.groupby(\"customer_ID\"))\n",
    "        )\n",
    "    df_below_mean_cc = pd.DataFrame(results).set_index(\"customer_ID\")\n",
    "    df_below_mean_cc.columns = [f\"{col}_below_mean_cc\" for col in df_below_mean_cc.columns]\n",
    "    \n",
    "    df_above_mean_agg = reduce_mem_usage(df_above_mean_agg, verbose=True)\n",
    "    df_below_mean_agg = reduce_mem_usage(df_below_mean_agg, verbose=True)\n",
    "    df_above_mean_cc = reduce_mem_usage(df_above_mean_cc, verbose=True)\n",
    "    df_below_mean_cc = reduce_mem_usage(df_below_mean_cc, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_above_mean_agg.shape[0]\n",
    "    all_results.append(df_above_mean_agg)\n",
    "    assert n_customers == df_below_mean_agg.shape[0]\n",
    "    all_results.append(df_below_mean_agg)\n",
    "    assert n_customers == df_above_mean_cc.shape[0]\n",
    "    all_results.append(df_above_mean_cc)\n",
    "    assert n_customers == df_below_mean_cc.shape[0]\n",
    "    all_results.append(df_below_mean_cc)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "\n",
    "    print(\"[INFO] Computing diff features\")\n",
    "    tic = time.time()\n",
    "    df_diff = df.groupby(\"customer_ID\")[num_features].diff()\n",
    "    df_diff[\"customer_ID\"] = df[\"customer_ID\"]\n",
    "    df_pchg = df.replace({0.:1e-10}).groupby(\"customer_ID\")[num_features].pct_change()\n",
    "    df_pchg[\"customer_ID\"] = df[\"customer_ID\"]\n",
    "\n",
    "    df_diff_agg = df_diff.groupby(\"customer_ID\")[num_features].agg([\"mean\",\"std\",\"min\",\"max\",])\n",
    "    df_diff_agg.columns = ['_diff_'.join(x) for x in df_diff_agg.columns]\n",
    "\n",
    "    df_pchg_agg = df_pchg.groupby(\"customer_ID\")[num_features].agg([\"mean\",\"std\",\"min\",\"max\"])\n",
    "    df_pchg_agg.columns = ['_pchg_'.join(x) for x in df_pchg_agg.columns]\n",
    "\n",
    "    df_diff_agg = reduce_mem_usage(df_diff_agg, verbose=True)\n",
    "    df_pchg_agg = reduce_mem_usage(df_pchg_agg, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_diff_agg.shape[0]\n",
    "    all_results.append(df_diff_agg)\n",
    "    assert n_customers == df_pchg_agg.shape[0]\n",
    "    all_results.append(df_pchg_agg)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "    \n",
    "    \n",
    "    print(\"[INFO] Computing diff/pchg lag features\")\n",
    "    tic = time.time()\n",
    "    # lags in diff features\n",
    "    df_diff_lag1 = df_diff.groupby(\"customer_ID\")[num_features].nth(-1, dropna=None)\n",
    "    df_diff_lag1.columns = [f\"{col}_diff_lag1\" for col in df_diff_lag1.columns]\n",
    "\n",
    "    df_diff_lag2 = df_diff.groupby(\"customer_ID\")[num_features].nth(-2, dropna=None)\n",
    "    df_diff_lag2.columns = [f\"{col}_diff_lag2\" for col in df_diff_lag2.columns]\n",
    "\n",
    "    df_diff_lag3 = df_diff.groupby(\"customer_ID\")[num_features].nth(-3, dropna=None)\n",
    "    df_diff_lag3.columns = [f\"{col}_diff_lag3\" for col in df_diff_lag3.columns]\n",
    "\n",
    "    df_diff_lag = pd.concat([df_diff_lag1, df_diff_lag2, df_diff_lag3], axis=1)\n",
    "\n",
    "    # lags on pct_change features\n",
    "    df_pchg_lag1 = df_pchg.groupby(\"customer_ID\")[num_features].nth(-1, dropna=None)\n",
    "    df_pchg_lag1.columns = [f\"{col}_pchg_lag1\" for col in df_pchg_lag1.columns]\n",
    "\n",
    "    df_pchg_lag2 = df_pchg.groupby(\"customer_ID\")[num_features].nth(-2, dropna=None)\n",
    "    df_pchg_lag2.columns = [f\"{col}_pchg_lag2\" for col in df_pchg_lag2.columns]\n",
    "\n",
    "    df_pchg_lag3 = df_pchg.groupby(\"customer_ID\")[num_features].nth(-3, dropna=None)\n",
    "    df_pchg_lag3.columns = [f\"{col}_pchg_lag3\" for col in df_pchg_lag3.columns]\n",
    "\n",
    "    df_pchg_lag = pd.concat([df_pchg_lag1, df_pchg_lag2, df_pchg_lag3], axis=1)\n",
    "\n",
    "    df_diff_lag = reduce_mem_usage(df_diff_lag, verbose=True)\n",
    "    df_pchg_lag = reduce_mem_usage(df_pchg_lag, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_diff_lag.shape[0]\n",
    "    all_results.append(df_diff_lag)\n",
    "    assert n_customers == df_pchg_lag.shape[0]\n",
    "    all_results.append(df_pchg_lag)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "    \n",
    "    \n",
    "    print(\"[INFO] computing complexity features\")\n",
    "    tic = time.time()\n",
    "    \n",
    "    df_diff_sq = df_diff[num_features]**2\n",
    "    df_diff_sq[\"customer_ID\"] = df_diff[\"customer_ID\"]\n",
    "\n",
    "    df_pchg_sq = df_pchg[num_features]**2\n",
    "    df_pchg_sq[\"customer_ID\"] = df_pchg[\"customer_ID\"]\n",
    "\n",
    "    df_cxty1 = df_diff_sq.groupby(\"customer_ID\")[num_features].agg([\"sum\",\"mean\"])\n",
    "    df_cxty1.columns = [\"_\".join(col)+\"_cxty1\" for col in df_cxty1.columns]\n",
    "    \n",
    "    df_cxty2 = df_pchg_sq.groupby(\"customer_ID\")[num_features].agg([\"sum\",\"mean\"])\n",
    "    df_cxty2.columns = [\"_\".join(col)+\"_cxty2\" for col in df_cxty2.columns]\n",
    "    \n",
    "    df_cxty1 = reduce_mem_usage(df_cxty1, verbose=True)\n",
    "    df_cxty2 = reduce_mem_usage(df_cxty2, verbose=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    assert n_customers == df_cxty1.shape[0]\n",
    "    all_results.append(df_cxty1)\n",
    "    assert n_customers == df_cxty2.shape[0]\n",
    "    all_results.append(df_cxty2)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "\n",
    "    print(\"[INFO] computing min_max_position features\")\n",
    "    tic = time.time()\n",
    "\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        delayed_func = delayed(min_max_position)\n",
    "        results = parallel(\n",
    "            delayed_func(_df, customer_ID, num_features) \n",
    "            for customer_ID,_df in tqdm(df.groupby(\"customer_ID\"))\n",
    "        )\n",
    "    df_min_max1 = pd.DataFrame(results).set_index(\"customer_ID\")\n",
    "\n",
    "    df_diff.rename(columns={col:col+\"_diff\" for col in num_features}, inplace=True)\n",
    "    num_features_diff = [col+\"_diff\" for col in num_features]\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        delayed_func = delayed(min_max_position)\n",
    "        results = parallel(\n",
    "            delayed_func(_df, customer_ID, num_features_diff) \n",
    "            for customer_ID,_df in tqdm(df_diff.groupby(\"customer_ID\"))\n",
    "        )\n",
    "    df_min_max2 = pd.DataFrame(results).set_index(\"customer_ID\")\n",
    "\n",
    "    df_min_max1 = reduce_mem_usage(df_min_max1, verbose=True)\n",
    "    df_min_max2 = reduce_mem_usage(df_min_max2, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_min_max1.shape[0]\n",
    "    all_results.append(df_min_max1)\n",
    "    assert n_customers == df_min_max2.shape[0]\n",
    "    all_results.append(df_min_max2)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "    \n",
    "    print(\"[INFO] Computing categorical aggregations\")\n",
    "    tic = time.time()\n",
    "\n",
    "    df_cat_agg1 = (\n",
    "        df\n",
    "        .groupby(\"customer_ID\")\n",
    "        [ohe_cols]\n",
    "        .mean()\n",
    "    )\n",
    "    df_cat_agg1.columns = [col+\"_\"+\"mean\" for col in df_cat_agg1.columns]\n",
    "\n",
    "    df_cat_agg2 = (\n",
    "        df\n",
    "        .groupby(\"customer_ID\")\n",
    "        [ohe_cols]\n",
    "        .agg(compute_last_observed)\n",
    "    )\n",
    "    df_cat_agg2.columns = [col+\"_\"+\"lo\" for col in df_cat_agg2.columns]\n",
    "\n",
    "    df_cat_agg1 = reduce_mem_usage(df_cat_agg1, verbose=True)\n",
    "    df_cat_agg2 = reduce_mem_usage(df_cat_agg2, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_cat_agg1.shape[0]\n",
    "    all_results.append(df_cat_agg1)\n",
    "    assert n_customers == df_cat_agg2.shape[0]\n",
    "    all_results.append(df_cat_agg2)\n",
    "    \n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "    \n",
    "    print(\"[INFO] Computing slope features\")\n",
    "    tic = time.time()\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        delayed_func = delayed(compute_slope_cols)\n",
    "        results = parallel(\n",
    "           delayed_func(_df, customer_ID, num_features) \n",
    "           for customer_ID,_df in tqdm(df.groupby(\"customer_ID\"))\n",
    "        )\n",
    "    slopes_df = pd.DataFrame(results).fillna(0).set_index(\"customer_ID\")\n",
    "    slopes_df.columns = [f\"{col}_slope\" for col in slopes_df.columns]\n",
    "\n",
    "    slopes_df = reduce_mem_usage(slopes_df, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == slopes_df.shape[0]\n",
    "    all_results.append(slopes_df)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "    \n",
    "    \n",
    "    print(\"[INFO] Building NaN related features\")\n",
    "    tic = time.time()\n",
    "\n",
    "    df_nan = df[num_features].isna()\n",
    "    df_nan[\"customer_ID\"] = df[\"customer_ID\"]\n",
    "\n",
    "    df_nan_agg = df_nan.groupby(\"customer_ID\")[num_features].agg([\"mean\", \"sum\"])\n",
    "    df_nan_agg.columns = [\"_\".join(col)+\"_\"+\"nan\" for col in df_nan_agg.columns]\n",
    "\n",
    "    df_nan_fpo = df_nan.groupby(\"customer_ID\")[num_features].agg(first_nan_position)\n",
    "    df_nan_fpo.columns = [col+\"_\"+\"nan_fpo\" for col in df_nan_fpo.columns]\n",
    "\n",
    "    df_nan_lpo = df_nan.groupby(\"customer_ID\")[num_features].agg(last_nan_position)\n",
    "    df_nan_lpo.columns = [col+\"_\"+\"nan_lpo\" for col in df_nan_lpo.columns]\n",
    "\n",
    "    df_nan_agg = reduce_mem_usage(df_nan_agg, verbose=True)\n",
    "    df_nan_fpo = reduce_mem_usage(df_nan_fpo, verbose=True)\n",
    "    df_nan_lpo = reduce_mem_usage(df_nan_lpo, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_nan_agg.shape[0]\n",
    "    all_results.append(df_nan_agg)\n",
    "    assert n_customers == df_nan_fpo.shape[0]\n",
    "    all_results.append(df_nan_fpo)\n",
    "    assert n_customers == df_nan_lpo.shape[0]\n",
    "    all_results.append(df_nan_lpo)\n",
    "    \n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "    \n",
    "    print(\"[INFO] Building S_2 related features\")\n",
    "    tic = time.time()\n",
    "    \n",
    "    df_count = df.groupby([\"customer_ID\"])[\"S_2\"].count()\n",
    "    df_count = pd.DataFrame(df_count).rename({\"S_2\":\"S_2_count\"}, axis=1)\n",
    "    \n",
    "    _tmp = df.groupby(\"customer_ID\").apply(compute_antiquity)\n",
    "    df_antiquity = pd.DataFrame(_tmp, columns=[\"S_2_antiquity\"])\n",
    "    \n",
    "    _tmp = df.groupby(\"customer_ID\")[\"S_2\"].apply(lambda x: x.diff().max().days)\n",
    "    df_max_gap = pd.DataFrame(_tmp).rename({\"S_2\":\"S_2_max_gap\"}, axis=1)\n",
    "\n",
    "    df_count = reduce_mem_usage(df_count, verbose=True)\n",
    "    df_antiquity = reduce_mem_usage(df_antiquity, verbose=True)\n",
    "    df_max_gap = reduce_mem_usage(df_max_gap, verbose=True)\n",
    "    gc.collect()\n",
    "\n",
    "    assert n_customers == df_count.shape[0]\n",
    "    all_results.append(df_count)\n",
    "    assert n_customers == df_antiquity.shape[0]\n",
    "    all_results.append(df_antiquity)\n",
    "    assert n_customers == df_max_gap.shape[0]\n",
    "    all_results.append(df_max_gap)\n",
    "\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "\n",
    "\n",
    "    print(\"[INFO] Concatenating all the results\")\n",
    "    tic = time.time()\n",
    "    output = pd.concat(all_results, axis=1)\n",
    "    tac = time.time()\n",
    "    print(f\"Elapsed time: {(tac-tic)/60} min\")\n",
    "    \n",
    "    del df\n",
    "    del df_num_agg, df_diff, df_pchg, df_diff_agg, df_pchg_agg, slopes_df\n",
    "    del df_min_max1, df_min_max2, df_cat_agg1, df_cat_agg2, df_count\n",
    "    gc.collect()\n",
    "\n",
    "    print('[INFO] shape after engineering', output.shape )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## preproc on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../data/ext/amex-data-integer-dtypes-parquet-format/train.parquet\")\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filter(re.compile(\"D_.*\").match, train.columns)))\n",
    "print(\"len:\", len(list(filter(re.compile(\"D_.*\").match, train.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filter(re.compile(\"S_.*\").match, train.columns)))\n",
    "print(\"len:\", len(list(filter(re.compile(\"S_.*\").match, train.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filter(re.compile(\"P_.*\").match, train.columns)))\n",
    "print(\"len:\", len(list(filter(re.compile(\"P_.*\").match, train.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filter(re.compile(\"B_.*\").match, train.columns)))\n",
    "print(\"len:\", len(list(filter(re.compile(\"B_.*\").match, train.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filter(re.compile(\"R_.*\").match, train.columns)))\n",
    "print(\"len:\", len(list(filter(re.compile(\"R_.*\").match, train.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = remove_noise(train)\n",
    "train, encoder, ohe_cols = encode_categoricals(train)\n",
    "train_agg = build_features(train, ohe_cols)\n",
    "train_agg = reduce_mem_usage(train_agg, verbose=True)\n",
    "train_agg.to_parquet(str(OUTPUT_PATH/\"train.parquet\"))\n",
    "\n",
    "del train,train_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## preproc on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test = pd.read_parquet(\"../data/ext/amex-data-integer-dtypes-parquet-format/test.parquet\")\n",
    "#test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#test = remove_noise(test)\n",
    "#test, _, _ = encode_categoricals(test, encoder)\n",
    "#test_agg = build_features(test, ohe_cols)\n",
    "#test_agg = reduce_mem_usage(test_agg, verbose=True)\n",
    "#test_agg.to_parquet(str(OUTPUT_PATH/\"test.parquet\"))\n",
    "\n",
    "#del test,test_agg\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
