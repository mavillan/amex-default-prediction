{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from metrics import compute_recall_at4, compute_normalized_gini, compute_amex_metric\n",
    "#from messaging import send_message\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics in xgboost format\n",
    "\n",
    "def metric_recall_at4(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    y_true = dtrain.get_label()\n",
    "    return 'recall_at4', compute_recall_at4(y_true, predt)\n",
    "\n",
    "def metric_normalized_gini(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    y_true = dtrain.get_label()\n",
    "    return 'norm_gini', compute_normalized_gini(y_true, predt)\n",
    "\n",
    "def metric_amex(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    y_true = dtrain.get_label()\n",
    "    return 'amex_metric', compute_amex_metric(y_true, predt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG PARAMS\n",
    "N_REPEATS = 3\n",
    "MAX_ITERATIONS = 3000\n",
    "DATASET_VERSION = \"02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOF_PATH = Path(f\"../data/oof/xgboost-gblinear-dsv{DATASET_VERSION}\")\n",
    "SUB_PATH = Path(f\"../data/subs/xgboost-gblinear-dsv{DATASET_VERSION}\")\n",
    "ART_PATH = Path(f\"../artifacts/xgboost-gblinear-dsv{DATASET_VERSION}\")\n",
    "\n",
    "if not OOF_PATH.exists():\n",
    "    OOF_PATH.mkdir(parents=True, exist_ok=True)\n",
    "if not SUB_PATH.exists():\n",
    "    SUB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "if not ART_PATH.exists():\n",
    "    ART_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(f\"../data/processed/dsv{DATASET_VERSION}/train.parquet\")\n",
    "train_labels = pd.read_csv(\"../data/raw/train_labels.csv\", index_col=\"customer_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_feats = train.columns.tolist()\n",
    "categ_feats = [\n",
    "    'B_30_first', 'B_38_first', 'D_114_first', 'D_116_first', 'D_117_first', \n",
    "    'D_120_first', 'D_126_first', 'D_63_first', 'D_64_first', 'D_66_first', 'D_68_first',\n",
    "    'B_30_last', 'B_38_last', 'D_114_last', 'D_116_last', 'D_117_last', \n",
    "    'D_120_last', 'D_126_last', 'D_63_last', 'D_64_last', 'D_66_last', 'D_68_last',\n",
    "]\n",
    "numeric_feats = [col for col in input_feats if col not in categ_feats]\n",
    "\n",
    "len(input_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_labels, how=\"inner\", left_index=True, right_index=True)\n",
    "del train_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## model training\n",
    "\n",
    "train with repeated cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    # general parameters\n",
    "    'booster':'gblinear',\n",
    "    'objective':'binary:logistic',\n",
    "    'disable_default_eval_metric':True,\n",
    "    'seed':2112,\n",
    "    'eta': 0.05,\n",
    "    # linear booster parameters\n",
    "    'updater': 'coord_descent',\n",
    "    'feature_selector': 'thrifty',\n",
    "    #'feature_selector': 'cyclic',\n",
    "    'top_k': 150,\n",
    "    #'alpha':  1.0,\n",
    "    #'lambda': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(dataframe: pd.DataFrame, n_folds: int = 5,) -> tuple:\n",
    "    \n",
    "    models = list()\n",
    "    encoders = list()\n",
    "    scalers = list()\n",
    "    \n",
    "    # dataframe to store the oof predictions\n",
    "    oof = dataframe[[\"target\"]].copy()\n",
    "    oof[\"pred\"] = -1\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        \n",
    "        print(f\" training model {fold+1}/{n_folds} \".center(100, \"#\"))\n",
    "        \n",
    "        train_df = dataframe.query(\"fold != @fold\").copy()\n",
    "        valid_df = dataframe.query(\"fold == @fold\").copy()\n",
    "        \n",
    "        encoder = ce.glmm.GLMMEncoder()\n",
    "        encoder.fit(train_df[categ_feats], train_df[\"target\"].values)\n",
    "        train_df[categ_feats] = encoder.transform(train_df[categ_feats])\n",
    "        valid_df[categ_feats] = encoder.transform(valid_df[categ_feats])\n",
    "        \n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        scaler.fit(train_df[input_feats].values)\n",
    "        train_df[input_feats] = scaler.transform(train_df[input_feats].values)\n",
    "        valid_df[input_feats] = scaler.transform(valid_df[input_feats].values)\n",
    "                        \n",
    "        train_dset = xgb.DMatrix(\n",
    "            data=train_df.loc[:,input_feats],\n",
    "            label=train_df.loc[:,\"target\"].values,\n",
    "        )\n",
    "        valid_dset = xgb.DMatrix(\n",
    "            data=valid_df.loc[:,input_feats],\n",
    "            label=valid_df.loc[:,\"target\"].values,\n",
    "        )\n",
    "        model = xgb.train(\n",
    "            params = model_params,\n",
    "            dtrain=train_dset,\n",
    "            num_boost_round=MAX_ITERATIONS,\n",
    "            early_stopping_rounds=int(0.1*MAX_ITERATIONS),\n",
    "            evals=[(valid_dset,\"eval\"), ],\n",
    "            custom_metric=metric_amex,\n",
    "            maximize=True,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "        \n",
    "        #lgb.plot_importance(model, figsize=(8,15), importance_type=\"split\", max_num_features=30)\n",
    "        #lgb.plot_importance(model, figsize=(8,15), importance_type=\"gain\", max_num_features=30)\n",
    "        #plt.show()        \n",
    "        \n",
    "        oof.loc[valid_df.index,\"pred\"] = model.predict(valid_dset)\n",
    "        \n",
    "        models.append(model)\n",
    "        encoders.append(encoder)\n",
    "        scalers.append(scaler)\n",
    "        del train_df,valid_df,train_dset,valid_dset\n",
    "        gc.collect()\n",
    "    \n",
    "    return models,encoders,scalers,oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement repeated cross validation\n",
    "sorted(glob(\"../data/processed/cv*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "all_models = list()\n",
    "all_encoders = list()\n",
    "all_scalers = list()\n",
    "all_oof_dfs = list()\n",
    "\n",
    "for repetition in range(N_REPEATS):\n",
    "    print(f\" repeated cross-validation step: {repetition+1}/{N_REPEATS} \".center(100, \"#\"))\n",
    "\n",
    "    folds = pd.read_csv(f'../data/processed/cv{repetition}.csv', index_col=\"customer_ID\")\n",
    "    _train = pd.merge(train, folds, how=\"inner\", left_index=True, right_index=True).reset_index(drop=True)\n",
    "    \n",
    "    tic = time.time()\n",
    "    models,encoders,scalers,oof = train_models(_train, n_folds=5)\n",
    "    tac = time.time()\n",
    "    print(f\"Training time: {(tac-tic)/60} min.\")\n",
    "          \n",
    "    # oof metrics\n",
    "    print(\"OOF recall_at4:\", compute_recall_at4(oof.target.values, oof.pred.values))\n",
    "    print(\"OOF normalized_gini:\", compute_normalized_gini(oof.target.values, oof.pred.values))\n",
    "    print(\"OOF competition metric:\", compute_amex_metric(oof.target.values, oof.pred.values))\n",
    "    \n",
    "    all_models.append(models)\n",
    "    all_encoders.append(encoders)\n",
    "    all_scalers.append(scalers)\n",
    "    all_oof_dfs.append(oof)\n",
    "    \n",
    "    # save oof predictions\n",
    "    oof.to_csv(OOF_PATH/f\"oof-cv{repetition}.csv\", index=False)\n",
    "    # save models\n",
    "    #for fold,_model in enumerate(models):\n",
    "    #    _model.save_model(ART_PATH/f\"/model-cv{repetition}-fold{fold}.cbm\", format=\"cbm\")\n",
    "          \n",
    "    del _train, folds; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "\n",
    "for oof in all_oof_dfs:  \n",
    "    r = {\n",
    "        \"recall_at4\": compute_recall_at4(oof.target.values, oof.pred.values),\n",
    "        \"gini\": compute_normalized_gini(oof.target.values, oof.pred.values),\n",
    "        \"metric\": compute_amex_metric(oof.target.values, oof.pred.values),\n",
    "    }\n",
    "    results.append(r)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "display(results)\n",
    "\n",
    "print(\"\\nmean:\")\n",
    "display(results.mean(axis=0))\n",
    "\n",
    "print(\"\\nstd:\")\n",
    "display(results.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## make predictions and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(\n",
    "        dataframe:pd.DataFrame, \n",
    "        input_feats:list,\n",
    "        categ_feats:list,\n",
    "        encoders:list,\n",
    "        scalers:list,\n",
    "        models:list,\n",
    "    ) -> np.array:\n",
    "    preds = list()\n",
    "    for encoder,scaler,model in zip(encoders,scalers,models):\n",
    "        _dataframe = dataframe.copy()\n",
    "        _dataframe[categ_feats] = encoder.transform(_dataframe[categ_feats])\n",
    "        _dataframe[input_feats] = scaler.transform(_dataframe[input_feats].values)\n",
    "        _dataframe_casted = xgb.DMatrix(data=_dataframe[input_feats])\n",
    "        preds.append(model.predict(_dataframe_casted))\n",
    "    return np.mean(preds, axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(f\"../data/processed/dsv{DATASET_VERSION}/test.parquet\")\n",
    "sub = pd.read_csv(\"../data/raw/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_preds = list()\n",
    "\n",
    "for repetition in range(N_REPEATS):\n",
    "    if \"prediction\" in sub.columns:\n",
    "        sub.drop(\"prediction\", axis=1, inplace=True)\n",
    "    if \"prediction\" in test.columns:\n",
    "        test.drop(\"prediction\", axis=1, inplace=True)\n",
    "        \n",
    "    models = all_models[repetition]\n",
    "    encoders = all_encoders[repetition]\n",
    "    preds = make_predictions(test, input_feats, categ_feats, encoders, scalers, models)\n",
    "    all_preds.append(preds)\n",
    "       \n",
    "    test[\"prediction\"] = preds\n",
    "    sub[\"prediction\"] = test.loc[sub.customer_ID.values,\"prediction\"].values\n",
    "    assert sub.prediction.isna().sum() == 0\n",
    "    sub.to_csv(SUB_PATH/f\"submission-cv{repetition}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# predict using all the trained models\n",
    "if \"prediction\" in sub.columns:\n",
    "    sub.drop(\"prediction\", axis=1, inplace=True)\n",
    "if \"prediction\" in test.columns:\n",
    "    test.drop(\"prediction\", axis=1, inplace=True)\n",
    "\n",
    "test[\"prediction\"] = np.mean(all_preds, axis=0)\n",
    "sub[\"prediction\"] = test.loc[sub.customer_ID.values,\"prediction\"].values\n",
    "assert sub.prediction.isna().sum() == 0\n",
    "sub.to_csv(SUB_PATH/f\"submission-all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
